import google.generativeai as genai
from app.configs.gemini_config import client_config
import json
import asyncio
import os

client = client_config

ANALYZE_GEMINI_PROMPT= """
# [Instruction]
ì§€ì •í•œ í˜•ì‹ì˜ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•˜ë¼, ì…ë ¥ëœ 'user_input'ì„ ë¶„ì„í•˜ì—¬

## [Information]
user_input = {user_input}

### [Output Format]
ë°˜ë“œì‹œ ë‹¤ìŒì— ì œì‹œëœ í‚¤ ìˆœì„œì™€ í˜•íƒœë¡œë§Œ ì´ë£¨ì–´ì§„ JSON ê°ì²´ë¥¼ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
â€¢ concept_tags: ì§ˆë¬¸ì—ì„œ íŒŒì•…í•  ì£¼ì œÂ·ê´€ì‹¬ì‚¬Â·í‚¤ì›Œë“œ ë°°ì—´ (ë‹¨, ì±…, ë„ì„œì™€ ê°™ì€ í‚¤ì›Œë“œ ì œì™¸)  
â€¢ user_intents: ë‹¤ìŒì˜ 2ê°€ì§€ê°€ ë°˜ì˜ëœ ë°°ì—´ (5ê°œ ì´ë‚´) 
1. 'user_input'ì˜ í•µì‹¬ ì˜ë„ 
2. ì‚¬ìš©ì í›„ì† ì§ˆì˜ë¡œ í™•ì¥ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê°œë…

#### [Strong Rules]
1. ì¶œë ¥ì€ ì˜¤ì§ JSON ë¬¸ë²•ì— ë§ëŠ” ê°ì²´ í•˜ë‚˜ë§Œ ìˆì–´ì•¼ í•˜ë©°, ì–´ë– í•œ ì¶”ê°€ ì„¤ëª…, ì£¼ì„, ë§ˆí¬ë‹¤ìš´ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
2. í‚¤ ìˆœì„œëŠ” ìœ„ ëª©ë¡ê³¼ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
3. JSONì— ëŒ€í•œ value ê°’ì€ 'í•œêµ­ì–´'ë¡œ ì¶œë ¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
"""

async def analyze_gemini(user_prompt):
    import logging
    
    prompt = ANALYZE_GEMINI_PROMPT.format(user_input=user_prompt)
    
    if client is None:
        logging.error("âŒ Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "llm error"
    
    try:
        # Gemini ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = client.GenerativeModel('gemini-2.5-flash-lite-preview-06-17')
        
        # ì»¨í…ì¸  ìƒì„±
        response = model.generate_content(prompt)
        
        # ì‹¤ì œ ì‘ë‹µ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        logging.info(f"ğŸ” Gemini ì›ë³¸ ì‘ë‹µ: {response.text}")
        
        return response.text
    except Exception as e:
        logging.error(f"âŒ Gemini API ì˜¤ë¥˜: {e}")
        return "llm error"
    
# --- 3. ì‹¤ì œ APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ ---
async def analyze_gemini_real(user_prompt: str) -> str | None:
    """ì‹¤ì œ Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜"""
    print(f"\n[INFO] Gemini APIì— ì‹¤ì œ ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤...")
    print(f"ã„´ ì…ë ¥ëœ ì§ˆë¬¸: '{user_prompt}'")
    
    try:
        # ì‚¬ìš©í•  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = genai.GenerativeModel('gemini-2.5-flash-lite-preview-06-17')
        
        # í”„ë¡¬í”„íŠ¸ í¬ë§·ì— ì‚¬ìš©ì ì§ˆë¬¸ ì‚½ì…
        prompt = ANALYZE_GEMINI_PROMPT.format(user_input=user_prompt)
        
        # APIë¥¼ í†µí•´ ì½˜í…ì¸  ìƒì„± ìš”ì²­ (ë¹„ë™ê¸° í˜¸ì¶œ)
        response = await model.generate_content_async(prompt)
        
        return response.text
        
    except Exception as e:
        print(f"\n[ERROR] API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# --- 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ë©”ì¸ ë¹„ë™ê¸° í•¨ìˆ˜ ---
async def main():
    # í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ ì§ˆë¬¸ì„ ì—¬ê¸°ì— ì…ë ¥í•©ë‹ˆë‹¤.
    sample_question = "í•œê°•ì˜ ì†Œì„¤ ì‘í’ˆì„ ì°¾ì•„ë³´ê³  ì‹¶ì–´"
    
    # ì‹¤ì œ API í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    result_str = await analyze_gemini_real(sample_question)
    
    print("\n-----------[ ì‹¤ì œ API ê²°ê³¼ ]-----------")
    if result_str:
        print(f"ê²°ê³¼ (Raw String):\n{result_str}")
    else:
        print("APIë¡œë¶€í„° ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
    print("----------------------------------------")

# --- 5. í”„ë¡œê·¸ë¨ ì‹¤í–‰ ---
if __name__ == "__main__":
    asyncio.run(main())
